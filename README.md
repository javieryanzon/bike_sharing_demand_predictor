<div align="center">
    <h1>Bike sharing demand predictor serviceüö≤üöõüìä</h1>
    <i>Proyecto finalüöÄ</i>
</div>

<br />
 

<i>*NOTA: El c√≥digo de proyecto no se est√° manteniendo por lo que es probable que no funcione correctamente</i>


## Predictor de demanda
1. El siguiente proyecto es un predictor de la demanda de un sistema de bikesharing. Est√° planteado como un proyecto final del M√°ster de Data Science de Nodd3r. Les comparto su [web](https://nodd3r.com/) y agradezco especialmente a Christian Donaire que me ayud√≥ mucho en todo este proceso de aprendizaje.

    Tambi√©n cabe destacar que este proyecto ha sido inspirado en el [curso](https://bit.ly/MLcourse_plb) de [Pau Labarta Bajo](https://github.com/Paulescu) en el que se contruye un predictor de demanda del servicio de taxi de Nueva York.

2. ¬øQue es un sistema de bikesharing? Es el sistema de bicicletas de modalidad compartidas, es decir que cada ciudadano puede usarlas y luego dejarlas en estaciones espec√≠ficas para ello. Estos sistemas est√°n presentes en distintas ciudades. En el siguiente proyecto se har√° un prototipo de predicci√≥n de demanda para planificar donde deber√≠an haber m√°s bicicletas en determinadas horas.

    Por tanto, el problema que se busca resolver es el rebalanceo de bicicletasüö≤‚û°Ô∏èüöõ en sistemas de bikesharing. Pero ¬øqu√© es el rebalanceo? B√°sicamente ser√≠a mover las bicicletas de una estaci√≥n a otra para que cuando vayas encuentres bicicletas para realizar tu viaje.

    Para ello se plantea predecir la demanda de bicicletas de las siguientes 36 horas. ¬øPara qu√© 36 horas? Para que la empresa que realiza ese trabajo pueda tener un tiempo considerable para preveer los picos de demanda.

3. ¬øC√≥mo? Basado en los datos de demanda de las horas anteriores se buscar√° predecir la demanda de las siguientes 36 horas.

    Para ello utilic√© el [dataset del Gobierno de la Cuidad de Buenos Aires](https://data.buenosaires.gob.ar/dataset/bicicletas-publicas) que se actualiza mensualmente para lograr este prop√≥sito. 
    
    Cabe destacar que utilic√© "poetry" para crear un entorno virtual y tener m√°s comodidad para gestionar librer√≠as. Adem√°s utilic√© un feature store llamado "hopsworks" con el que guardo los datos hist√≥ricos, el modelo creado y las predicciones.

    Tambi√©n utilic√© github actions para automatizar el script de descarga de features de la web del gobierno de buenos aires y la subida a hopsworks. Tambi√©n hice lo mismo con las predicciones, es decir un script que cada hora predice, y sube a hopsworks esa predicci√≥n. Esto fue hecho para que el tablero s√≥lo tenga que consumir esos datos que est√°n guardados y sea m√°s r√°pido.

4. ¬øQu√© modeloü§ñ se utiliz√≥ para ello? Los modelos basados en XGBoost son muy √∫tiles para predecir series de tiempo (y adem√°s mucho menos complejos que una red neuronal) pero para que funcionen correctamente se le debe dar los datos de una determinada manera que le facilita el aprendizaje. 


## Resumen del c√≥digo
1. En el notebook 1, 2, 3, 4 y 5 b√°sicamente lo que se hizo fue:
    - Descargar los datos y descomprimirlos.
    - Realizar una limpieza y convertirlos a formato parquet dado que es un formato que es √∫til para el prop√≥sito que buscamos y tiene varias ventajas.
    - Eliminar los minutos y segundos y aproximarlos a la hora previa.
    - Agregar las horas que no hubieron viajes con el valor "cero" y graficar.
    - Crear una funci√≥n en la que obtenemos los √≠ndices de las distintas filas para luego darle la forma m√°s adecuada al dataset para que el modelo aprenda.
    - Crear ese dataset que el modelo utilizar√° para aprender. (La forma en la que transformamos el dataset es que pasan de ser 3 columnas con hora, viaje y estaci√≥n, a una columna por cada hora, junto con la informaci√≥n de la estaci√≥n y la hora de referencia. Es decir del dataset original tomamos una cantidad de filas (horas previas y siguientes) y realizamos una transposici√≥n, luego bajamos una fila y repetimos el proceso. En este caso utilizamos 672 horas previas es decir 28 d√≠as y 36 horas siguientes).
    - Por √∫ltimo, se realiz√≥ una funci√≥n que grafique los registros previos y los siguientes.

 2. En el notebook 6, 7, 8, 9 y 10:
    - Se divieron los datos en train y test.
    - Se crearon un modelos base (sin aplicar Machine Learning) sobre los que comparar luego los modelos m√°s complejos.
    - Luego se prob√≥ con XGBoost y LightGBM dando mejores resultados √©ste √∫ltimo.
    - Lo siguiente fue seguir con LightGBM y aplicarle feature engineering para mejorar el modelo. Para ello se agreg√≥: el promedio de las √∫ltimas 4 semanas, latitud y longitud, hora y d√≠a de la semana.
    - Se utiliz√≥ optuna para realizar un hyperparameter tuning del modelo.

 3. En el notebook 11, 12, 13 y 14:
    - Se cre√≥ el proyecto en Hopsworks (feature store). Lo cual nos permite ir guardando los distintos registros que se descargan. Para ello se debe crear un feature group en el que guardarlo y luego para poder consumirlo es m√°s c√≥modo mediante un feature view. Para ello se van creando este tipo de figuras para poder guardar los datos.
    - El notebook 12 b√°sicamente lo que realiza es: descarga los datos de la web del Gobierno de Bs As, realiza una limpieza y lo sube al feature store. Para automatizar esto se utiliz√≥ un github action que se ejecuta cada hora.
    - En el notebook 13 se obtiene el modelo, se guarda y se sube a CometML (que luego se lo utilizar√° para realizar las predicciones).
    - En el notebook 14 se leen los distintos datos del feature store, se carga el modelo, se crean las predicciones y se las guarda en el feature store. Para automatizarlo se creo otra github action que se ejecuta inmediatamente despu√©s de que termina la otra github action.

 4. Tambi√©n tenemos otros archivos en las carpeta src. En ellas hay distintas funciones que se utilizan en los distintos notebooks para no tener que repetir la funci√≥n. Por tanto s√≥lo con importarla ya se puede utilizar. Adem√°s dentro de esa carpeta est√°n los dos tableros que ahora comentar√©:
    - El primer tablero es el de frontend el cual consulta al feature store y carga los datos previos y las predicciones correspondientes. Adem√°s se grafica un mapa en el que se puede ver la estaci√≥n que tendr√° m√°s demanda en las pr√≥ximas 36 horas (en la descripci√≥n est√° la demanda esperada y la hora). Luego m√°s abajo se encontrar√°n los gr√°ficos que del top 10 de estaciones con la m√°xima demanda.
    - El segundo tabledo es el de frontend monitoring en el que se puede observar el error global y el error de las estaciones con mayor demanda.

## Tableros
- [Dashboard con predicciones del modeloüìà](https://bike-sharing-demand-predictor-ecobici.streamlit.app/)

<p align="center">
<img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExOXh6ajJvaHZ4ZWlidmpqaWV6amY0ejJvcDBuNjN6ZXZzemllaThkNSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/0Bw3NBLM3mMmzILD7x/giphy.gif" width="500" align="center">
</p>
<br />

- [Dashboard con monitorizaci√≥n de errores del modeloüîç](https://bike-sharing-mae-error-monitoring.streamlit.app/)

<p align="center">
<img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdHBxcjl4cno0eW5wc211ZXhtYWIwdTljYXp3Y3V0bHplcnB2MzZzOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/B6CSUxVP4r35Qr0MCI/giphy.gif" width="500" align="center">
</p>

 <br />
 PD1: Cabe destacar que no se tiene acceso a los datos reales de la √∫ltima hora. Por tanto para salvar eso, lo que se hace es una simulaci√≥n de consulta en la que se obtienen datos de otro a√±o que simulan ser la √∫ltima hora, para luego incluirlos en la base de datos.

 PD2: En caso de que al abrir los tableros aparezca un error, volver a cargar la p√°gina para solucionarlo.

<br />
<div align="center">
    <i>Gracias por leer. Sigamos en contactoüôåüèª</i>
    <br />
    <a href="https://twitter.com/javieryanzon">Twitter</a> ‚Ä¢
    <a href="https://www.linkedin.com/in/javieryanzon">LinkedIn</a>
<br />
</div>

 
