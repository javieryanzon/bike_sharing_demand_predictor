{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=Timestamp('2025-08-10 21:00:00+0000', tz='UTC')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-08-10 21:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.to_datetime(datetime.utcnow(), utc=True).floor('H')\n",
    "print(f'{current_date=}')\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for: 2025-08-10 21:00:00+00:00\n",
      "2025-08-10 23:08:13,662 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-08-10 23:08:13,666 INFO: Initializing external client\n",
      "2025-08-10 23:08:13,667 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-08-10 23:08:14,763 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/100501\n",
      "‚úÖ Feature group found: time_series_hourly_feature_group\n",
      "üìä Schema: ['pickup_hour', 'rides', 'pickup_location_id']...\n",
      "2025-08-10 23:08:34,972 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n",
      "‚ö†Ô∏è Direct read failed: Could not read data using Hopsworks Query Service.\n",
      "üîÑ Trying feature view approach...\n",
      "Fetching from 2025-07-13 21:00:00+00:00 to 2025-08-10 20:00:00+00:00\n",
      "Initializing batch scoring...\n",
      "Trying config 1/3...\n",
      "2025-08-10 23:08:44,885 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:44.8656913+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_18752\\3710731520.py\", line 32, in load_features_robust\n",
      "    data = fg.read()\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\feature_group.py\", line 2597, in read\n",
      "    return self.select_all().read(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\constructor\\query.py\", line 309, in read\n",
      "    return engine.get_instance().sql(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 146, in sql\n",
      "    return self._sql_offline(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 189, in _sql_offline\n",
      "    result_df = util.run_with_loading_animation(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hopsworks_common\\util.py\", line 373, in run_with_loading_animation\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 412, in afs_error_handler_wrapper\n",
      "    raise FeatureStoreException(user_message) from e\n",
      "hopsworks_common.client.exceptions.FeatureStoreException: Could not read data using Hopsworks Query Service.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:44.8656913+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n",
      "‚ö†Ô∏è Config 1 failed: Could not read data using Hopsworks Query Service.\n",
      "Trying config 2/3...\n",
      "2025-08-10 23:08:54,391 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:54.3734194+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_18752\\3710731520.py\", line 32, in load_features_robust\n",
      "    data = fg.read()\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\feature_group.py\", line 2597, in read\n",
      "    return self.select_all().read(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\constructor\\query.py\", line 309, in read\n",
      "    return engine.get_instance().sql(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 146, in sql\n",
      "    return self._sql_offline(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 189, in _sql_offline\n",
      "    result_df = util.run_with_loading_animation(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hopsworks_common\\util.py\", line 373, in run_with_loading_animation\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 412, in afs_error_handler_wrapper\n",
      "    raise FeatureStoreException(user_message) from e\n",
      "hopsworks_common.client.exceptions.FeatureStoreException: Could not read data using Hopsworks Query Service.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:54.3734194+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n",
      "‚ö†Ô∏è Config 2 failed: Could not read data using Hopsworks Query Service.\n",
      "Trying config 3/3...\n",
      "2025-08-10 23:09:06,267 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:09:06.2498685+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_18752\\3710731520.py\", line 32, in load_features_robust\n",
      "    data = fg.read()\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\feature_group.py\", line 2597, in read\n",
      "    return self.select_all().read(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\constructor\\query.py\", line 309, in read\n",
      "    return engine.get_instance().sql(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 146, in sql\n",
      "    return self._sql_offline(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py\", line 189, in _sql_offline\n",
      "    result_df = util.run_with_loading_animation(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hopsworks_common\\util.py\", line 373, in run_with_loading_animation\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 412, in afs_error_handler_wrapper\n",
      "    raise FeatureStoreException(user_message) from e\n",
      "hopsworks_common.client.exceptions.FeatureStoreException: Could not read data using Hopsworks Query Service.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"c:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1633, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:09:06.2498685+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n",
      "‚ö†Ô∏è Config 3 failed: Could not read data using Hopsworks Query Service.\n",
      "‚ùå Feature group error: All feature view configs failed\n",
      "‚ùå Connection error: All feature view configs failed\n",
      "\n",
      "‚ùå FAILED TO LOAD FEATURES: All feature view configs failed\n",
      "\n",
      "üí° NEXT STEPS:\n",
      "1. Run the feature pipeline first: notebooks/12_feature_pipeline.ipynb\n",
      "2. Check if data exists in Hopsworks UI\n",
      "3. Verify your feature group schema matches your data\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "All feature view configs failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFlightServerError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:394\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:459\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[1;34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[0m\n\u001b[0;32m    458\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrow_flight_config\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py:55\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Retrying(\u001b[38;5;241m*\u001b[39mdargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdkw)\u001b[38;5;241m.\u001b[39mcall(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py:279\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reject(attempt):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(attempt)\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py:326\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    325\u001b[0m         exc_type, exc, tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\retrying.py:273\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:445\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[1;34m(self, descriptor, timeout, dataframe_type)\u001b[0m\n\u001b[0;32m    442\u001b[0m options \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightCallOptions(\n\u001b[0;32m    443\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_certificates_headers()\n\u001b[0;32m    444\u001b[0m )\n\u001b[1;32m--> 445\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mticket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe_type)\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\pyarrow\\_flight.pyx:1633\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.do_get\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\pyarrow\\_flight.pyx:58\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFlightServerError\u001b[0m: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n    result = func(instance, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 196, in do_get\n    return self._read_query(context, path, command)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n    return func(instance, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n    result_batches = self.hudi_query_engine.read_query(query_obj)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n    self.open(\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n    f = self._open(\n        ^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n    stream = method(path, **_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n    raise FlyingDuckException(str(e)) from e\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/bike_sharing_demand_featurestore.db/time_series_hourly_feature_group_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-08-10T21:08:34.9558379+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mload_features_robust\u001b[1;34m(current_date)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# M√©todo 1: Leer directamente sin limit\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows directly from feature group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\feature_group.py:2597\u001b[0m, in \u001b[0;36mFeatureGroup.read\u001b[1;34m(self, wallclock_time, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m   2596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\constructor\\query.py:309\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         )\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow_flight_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\engine\\python.py:189\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhsfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[1;32m--> 189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_loading_animation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hopsworks_common\\util.py:373\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[1;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\jayan\\Desktop\\Python, SQL, Power Bi, cursos\\Proyectos\\bike_sharing_demand_predictor\\.venv\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:412\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m: Could not read data using Hopsworks Query Service.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Ejecutar\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mload_features_robust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéØ FEATURES LOADED SUCCESSFULLY!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m, in \u001b[0;36mload_features_robust\u001b[1;34m(current_date)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Direct read failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(read_error)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;66;03m# M√©todo 2: Usar feature view si el FG directo falla\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_via_feature_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fg_error:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Feature group error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(fg_error)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m, in \u001b[0;36mload_via_feature_view\u001b[1;34m(fs, current_date)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Config \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll feature view configs failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: All feature view configs failed"
     ]
    }
   ],
   "source": [
    "# Versi√≥n robusta para cargar features\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import hopsworks\n",
    "import src.config as config\n",
    "\n",
    "def load_features_robust(current_date):\n",
    "    \"\"\"Cargar features de manera robusta\"\"\"\n",
    "    print(f\"Loading features for: {current_date}\")\n",
    "    \n",
    "    try:\n",
    "        # Conectar usando config.py (no variables de entorno)\n",
    "        project = hopsworks.login(\n",
    "            project=config.HOPSWORKS_PROJECT_NAME,\n",
    "            api_key_value=config.HOPSWORKS_API_KEY\n",
    "        )\n",
    "        \n",
    "        fs = project.get_feature_store()\n",
    "        \n",
    "        # Primero, verificar qu√© feature groups existen\n",
    "        try:\n",
    "            fg = fs.get_feature_group(\n",
    "                name=config.FEATURE_GROUP_NAME,\n",
    "                version=config.FEATURE_GROUP_VERSION\n",
    "            )\n",
    "            print(f\"‚úÖ Feature group found: {fg.name}\")\n",
    "            print(f\"üìä Schema: {[f.name for f in fg.features][:10]}...\") # Primeras 10 columnas\n",
    "            \n",
    "            # Intentar leer datos del feature group directamente\n",
    "            try:\n",
    "                # M√©todo 1: Leer directamente sin limit\n",
    "                data = fg.read()\n",
    "                print(f\"‚úÖ Read {len(data)} rows directly from feature group\")\n",
    "                print(f\"Columns: {list(data.columns)}\")\n",
    "                print(f\"Date range: {data['pickup_hour'].min()} to {data['pickup_hour'].max()}\")\n",
    "                return data\n",
    "                \n",
    "            except Exception as read_error:\n",
    "                print(f\"‚ö†Ô∏è Direct read failed: {str(read_error)}\")\n",
    "                \n",
    "                # M√©todo 2: Usar feature view si el FG directo falla\n",
    "                return load_via_feature_view(fs, current_date)\n",
    "                \n",
    "        except Exception as fg_error:\n",
    "            print(f\"‚ùå Feature group error: {str(fg_error)}\")\n",
    "            raise\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_via_feature_view(fs, current_date):\n",
    "    \"\"\"Cargar datos usando feature view como fallback\"\"\"\n",
    "    print(\"üîÑ Trying feature view approach...\")\n",
    "    \n",
    "    fv = fs.get_feature_view(\n",
    "        name=config.FEATURE_VIEW_NAME,\n",
    "        version=config.FEATURE_VIEW_VERSION\n",
    "    )\n",
    "    \n",
    "    # Calcular fechas\n",
    "    fetch_data_from = current_date - timedelta(days=28)\n",
    "    fetch_data_to = current_date - timedelta(hours=1)\n",
    "    \n",
    "    print(f\"Fetching from {fetch_data_from} to {fetch_data_to}\")\n",
    "    \n",
    "    # Inicializar batch scoring\n",
    "    if hasattr(fv, '_batch_scoring_server') and not fv._batch_scoring_server._serving_initialized:\n",
    "        print(\"Initializing batch scoring...\")\n",
    "        fv.init_batch_scoring()\n",
    "    \n",
    "    # Intentar con diferentes configuraciones\n",
    "    configs = [\n",
    "        {\"arrow_flight_config\": {\"timeout\": 300}},\n",
    "        {\"arrow_flight_config\": {\"timeout\": 180}},\n",
    "        {}\n",
    "    ]\n",
    "    \n",
    "    for i, read_options in enumerate(configs):\n",
    "        try:\n",
    "            print(f\"Trying config {i+1}/3...\")\n",
    "            ts_data = fv.get_batch_data(\n",
    "                start_time=pd.to_datetime(fetch_data_from - timedelta(days=1), utc=True),\n",
    "                end_time=pd.to_datetime(fetch_data_to + timedelta(days=1), utc=True),\n",
    "                read_options=read_options\n",
    "            )\n",
    "            print(f\"‚úÖ Success with config {i+1}: {len(ts_data)} rows\")\n",
    "            return ts_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Config {i+1} failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"All feature view configs failed\")\n",
    "\n",
    "# Ejecutar\n",
    "try:\n",
    "    features = load_features_robust(current_date)\n",
    "    print(\"\\nüéØ FEATURES LOADED SUCCESSFULLY!\")\n",
    "    print(f\"Shape: {features.shape}\")\n",
    "    print(f\"Columns: {list(features.columns)[:10]}...\")  # Primeras 10\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå FAILED TO LOAD FEATURES: {str(e)}\")\n",
    "    print(\"\\nüí° NEXT STEPS:\")\n",
    "    print(\"1. Run the feature pipeline first: notebooks/12_feature_pipeline.ipynb\")\n",
    "    print(\"2. Check if data exists in Hopsworks UI\")\n",
    "    print(\"3. Verify your feature group schema matches your data\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar informaci√≥n sobre las features cargadas\n",
    "if 'features' in locals():\n",
    "    print(\"üìä FEATURES SUMMARY:\")\n",
    "    print(f\"Shape: {features.shape}\")\n",
    "    print(f\"Columns: {list(features.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(features.head())\n",
    "    \n",
    "    print(f\"\\nDate info:\")\n",
    "    if 'pickup_hour' in features.columns:\n",
    "        print(f\"Date range: {features['pickup_hour'].min()} to {features['pickup_hour'].max()}\")\n",
    "        print(f\"Unique dates: {features['pickup_hour'].nunique()}\")\n",
    "    \n",
    "    if 'pickup_location_id' in features.columns:\n",
    "        print(f\"Unique locations: {features['pickup_location_id'].nunique()}\")\n",
    "else:\n",
    "    print(\"‚ùå Features not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo y hacer predicciones (solo si features est√°n disponibles)\n",
    "if 'features' in locals() and len(features) > 0:\n",
    "    try:\n",
    "        from src.model_registry_api import get_latest_model_from_registry\n",
    "        from src.inference import get_model_predictions\n",
    "\n",
    "        print(\"ü§ñ Loading model...\")\n",
    "        model = get_latest_model_from_registry(\n",
    "            model_name='bike_demand_predictor_next_hour', \n",
    "            status='Production'\n",
    "        )\n",
    "        \n",
    "        print(\"üîÆ Making predictions...\")\n",
    "        predictions = get_model_predictions(model, features)\n",
    "        predictions['pickup_hour'] = current_date\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(predictions)} predictions\")\n",
    "        display(predictions.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model prediction failed: {str(e)}\")\n",
    "        # Crear predicciones dummy para testing\n",
    "        print(\"üîß Creating dummy predictions for testing...\")\n",
    "        predictions = pd.DataFrame({\n",
    "            'pickup_location_id': features['pickup_location_id'],\n",
    "            'predicted_demand': [1.0] * len(features),  # Dummy predictions\n",
    "            'pickup_hour': current_date\n",
    "        })\n",
    "        print(f\"Created {len(predictions)} dummy predictions\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot make predictions without features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar predicciones en feature store (solo si existen)\n",
    "if 'predictions' in locals():\n",
    "    try:\n",
    "        from src.feature_store_api import get_feature_store\n",
    "        import src.config as config\n",
    "\n",
    "        print(\"üíæ Saving predictions to feature store...\")\n",
    "        \n",
    "        # Conectar al feature store\n",
    "        feature_group = get_feature_store().get_or_create_feature_group(\n",
    "            name=config.FEATURE_GROUP_MODEL_PREDICTIONS,\n",
    "            version=1,\n",
    "            description=\"Predictions generated by our production model\",\n",
    "            primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "            event_time='pickup_hour',\n",
    "        )\n",
    "        \n",
    "        # Insertar predicciones\n",
    "        job, _ = feature_group.insert(\n",
    "            predictions, \n",
    "            write_options={\"wait_for_job\": False}\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Predictions saved successfully!\")\n",
    "        print(f\"Job URL: {job.get_url() if hasattr(job, 'get_url') else 'N/A'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save predictions: {str(e)}\")\n",
    "        print(\"‚ö†Ô∏è This might be OK for testing purposes\")\n",
    "else:\n",
    "    print(\"‚ùå No predictions to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-py3.9 (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
